import os
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import LatentDirichletAllocation, PCA
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from transformers import AutoTokenizer, AutoModel
import torch
import shutil

# --- 1. Setup and Data Loading ---
folder_path = "/Users/evehuang/Downloads/453_doc"

def load_texts_from_folder(folder_path):
    documents = []
    filenames = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".txt"):
            filepath = os.path.join(folder_path, filename)
            with open(filepath, "r", encoding="utf-8") as f:
                documents.append(f.read())
                filenames.append(filename)
    # Sort to ensure consistent order
    sorted_pairs = sorted(zip(filenames, documents))
    sorted_filenames, sorted_documents = zip(*sorted_pairs)
    return list(sorted_documents), list(sorted_filenames)

documents, filenames = load_texts_from_folder(folder_path)
print(f"\nLoaded {len(documents)} documents from '{folder_path}'.")
print("Document Order:", filenames) # To help match documents to topics

# 1. TF-IDF Vectorization (avoiding stopwords, lemmatization, stemming)
tfidf_vectorizer = TfidfVectorizer(
    lowercase=True,
    token_pattern=r'\b\w+\b',
    stop_words='english',
    ngram_range=(1, 1)
)

tfidf_matrix = tfidf_vectorizer.fit_transform(documents)
feature_names = tfidf_vectorizer.get_feature_names_out()

print(f"\nTF-IDF matrix shape: {tfidf_matrix.shape}")

# --- 2. Topic Modeling ---

## Latent Dirichlet Allocation (LDA)
n_components_lda = 2 # Number of topics for LDA
lda_model = LatentDirichletAllocation(
    n_components=n_components_lda,
    random_state=42,
    learning_method='batch',
    n_jobs=-1
)
lda_topic_distributions = lda_model.fit_transform(tfidf_matrix)
lda_dominant_topics = np.argmax(lda_topic_distributions, axis=1)

print("\n--- LDA Topics (Top Words) ---")
def display_topics(model, feature_names, no_top_words):
    topics_list = []
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
        topics_list.append(top_words)
        print(f"Topic {topic_idx + 1}: {' '.join(top_words)}")
    return topics_list

lda_top_words = display_topics(lda_model, feature_names, 10)
print(f"LDA Dominant Topics for documents: {lda_dominant_topics}")

## KMeans Clustering on TF-IDF Vectors
n_clusters_tfidf = 2 # Number of clusters for TF-IDF based KMeans
kmeans_tfidf = KMeans(n_clusters=n_clusters_tfidf, random_state=42, n_init=10)
kmeans_tfidf_labels = kmeans_tfidf.fit_predict(tfidf_matrix)

print(f"\nKMeans (TF-IDF) Clusters/Topics: {kmeans_tfidf_labels}")

## BERT for Topic Modeling (Clustering on BERT Embeddings)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

def get_bert_embeddings(texts, tokenizer, model, device="cpu"):
    model.to(device)
    model.eval()
    embeddings = []
    batch_size = 8 # Process in batches to save memory
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(batch_texts, return_tensors="pt", truncation=True, padding=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        # Using [CLS] token embedding for sentence representation
        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        embeddings.extend(batch_embeddings)
    return np.array(embeddings)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"\nUsing device for BERT embeddings: {device}")

bert_embeddings = get_bert_embeddings(documents, tokenizer, model, device)
print(f"BERT embeddings shape: {bert_embeddings.shape}")

n_clusters_bert = 2 # Number of clusters for BERT based KMeans
kmeans_bert = KMeans(n_clusters=n_clusters_bert, random_state=42, n_init=10)
bert_cluster_labels = kmeans_bert.fit_predict(bert_embeddings)

print(f"BERT (KMeans) Clusters/Topics: {bert_cluster_labels}")

# --- 3. Visualize Topic Results ---

# Prepare data for visualization
results_df = pd.DataFrame({
    'filename': filenames,
    'document': documents,
    'lda_topic': lda_dominant_topics,
    'kmeans_tfidf_topic': kmeans_tfidf_labels,
    'kmeans_bert_topic': bert_cluster_labels
})

print("\n--- Document Topics Summary ---")
print(results_df)

# Dimensionality Reduction for Visualization
# PCA for TF-IDF
pca_tfidf = PCA(n_components=2, random_state=42)
tfidf_2d = pca_tfidf.fit_transform(tfidf_matrix.toarray())

# PCA for LDA (use the topic distribution itself for plotting)
lda_2d = lda_topic_distributions[:, :2]

# t-SNE for BERT embeddings
tsne_bert = TSNE(n_components=2, random_state=42, perplexity=min(5, len(documents) - 1))
bert_2d = tsne_bert.fit_transform(bert_embeddings)

# Plotting function
def plot_topics(data_2d, labels, title, method_name, ax, custom_palette=None):
    if custom_palette is None:
        custom_palette = sns.color_palette("viridis", n_colors=len(np.unique(labels)))

    # Create a DataFrame for Seaborn plotting
    plot_df = pd.DataFrame(data_2d, columns=['Component 1', 'Component 2'])
    plot_df['Topic'] = labels
    plot_df['Filename'] = [f.replace(".txt", "") for f in filenames]

    sns.scatterplot(
        x='Component 1',
        y='Component 2',
        hue='Topic',
        data=plot_df,
        palette=custom_palette,
        s=150, # Marker size
        alpha=0.8,
        edgecolor='k',
        legend='full',
        ax=ax
    )
    ax.set_title(f'{title}\n({method_name})', fontsize=14)
    ax.set_xlabel('Component 1')
    ax.set_ylabel('Component 2')
    ax.grid(True, linestyle='--', alpha=0.6)

    # Annotate points with filenames for better understanding
    for i, txt in enumerate(plot_df['Filename']):
        ax.text(data_2d[i, 0] + 0.05, data_2d[i, 1] + 0.05, txt,
                fontdict={'size': 8, 'weight': 'bold'}, alpha=0.7)


# --- Create and show plots ---
fig, axes = plt.subplots(1, 3, figsize=(24, 8)) # Increased figure size
fig.suptitle('Topic Modeling Comparison Across Methods', fontsize=20, y=1.02) # Adjusted title position

n_total_topics = max(n_components_lda, n_clusters_tfidf, n_clusters_bert)
color_palette = sns.color_palette("tab10", n_total_topics) # A categorical palette

# Plot LDA
plot_topics(lda_2d, results_df['lda_topic'], 'LDA Topics', 'LDA', axes[0], custom_palette=color_palette)

# Plot KMeans on TF-IDF
plot_topics(tfidf_2d, results_df['kmeans_tfidf_topic'], 'KMeans Topics', 'TF-IDF + KMeans', axes[1], custom_palette=color_palette)

# Plot KMeans on BERT Embeddings
plot_topics(bert_2d, results_df['kmeans_bert_topic'], 'KMeans Topics', 'BERT Embeddings + KMeans', axes[2], custom_palette=color_palette)

plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to prevent title overlap
plt.show()
